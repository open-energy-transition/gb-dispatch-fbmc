# SPDX-FileCopyrightText: gb-dispatch-model contributors
#
# SPDX-License-Identifier: MIT

"""
Compose the Great Britain focused PyPSA network described in
``doc/gb-model/index.rst``.

The rule assembles the clustered PyPSA-Eur base network with GB-specific
artefacts (manual region shapes, neighbouring countries, adjusted grid
connection costs) so that downstream rules can import a consistent
``networks/composed_{clusters}.nc`` snapshot.
"""

import copy
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import pandas as pd
import pypsa
import xarray as xr

from scripts._helpers import configure_logging, set_scenario_config
from scripts.add_electricity import (
    add_missing_carriers,
    attach_conventional_generators,
    attach_hydro,
    flatten,
)
from scripts.gb_model._helpers import get_lines

logger = logging.getLogger(__name__)


@dataclass
class CompositionContext:
    """Context for network composition containing paths and configuration."""

    resources_root: Path
    countries: tuple[str, ...]
    costs_path: Path
    costs_config: dict[str, Any]
    max_hours: dict[str, Any] | None


def create_context(
    network_path: str,
    costs_path: str,
    countries: list[str],
    costs_config: dict[str, Any],
    max_hours: dict[str, Any] | None,
) -> CompositionContext:
    """
    Create composition context from network path and configuration.

    Parameters
    ----------
    network_path : str
        Path to the input network file
    costs_path : str
        Path to the costs CSV file
    countries : list[str]
        List of country codes to include
    costs_config : dict
        Costs configuration dictionary
    max_hours : dict or None
        Maximum hours configuration

    Returns
    -------
    CompositionContext
        Context object with paths and configuration
    """
    resources_root = Path(network_path).parents[1]

    return CompositionContext(
        resources_root=resources_root,
        countries=tuple(countries),
        costs_path=Path(costs_path),
        costs_config=copy.deepcopy(costs_config),
        max_hours=max_hours,
    )


def _input_list_to_dict(input_list: list[str], parent: bool = False) -> dict[str, str]:
    """
    Convert a list of input file paths to a dictionary mapping types to paths.

    Args:
        input_list (list[str]): List of input file paths.
        parent (bool): Whether to use the parent directory name as the key.

    Returns:
        dict[str, str]: Dictionary mapping input types to file paths.
    """
    input_dict = {}
    for input_path in input_list:
        key = (Path(input_path).parent if parent else Path(input_path)).stem
        input_dict[key] = input_path
    return input_dict


def _add_timeseries_data_to_network(
    pypsa_t_dict: dict, data: pd.DataFrame, attribute: str
) -> None:
    """
    Add/update timeseries data to a network attribute.

    This is a robust approach to add data to the network when it is not known whether there is already data attached to the attribute.
    Any existing columns in the network attribute that are also in the incoming data will be overwritten.
    All other columns will remain as-is.

    Args:
        pypsa_t_dict (dict): PyPSA network timeseries component dictionary (e.g., n.loads_t).
        data (pd.DataFrame): Timeseries data to add.
        attribute (str): Network timeseries attribute to update.
    """
    assert pypsa_t_dict[attribute].index.equals(data.index), (
        f"Snapshot indices do not match between network attribute {attribute} and data being added."
    )
    logger.info(
        "Updating network timeseries attribute '%s' with %d columns of data.",
        attribute,
        len(data.columns),
    )
    pypsa_t_dict[attribute] = (
        pypsa_t_dict[attribute]
        .loc[:, ~pypsa_t_dict[attribute].columns.isin(data.columns)]
        .join(data)
    )


def _load_powerplants(
    powerplants_path: str,
    year: int,
) -> pd.DataFrame:
    """
    Load powerplant data.

    Parameters
    ----------
    powerplants_path : str
        Path to powerplants CSV file
    year : int
        Year to filter powerplants

    Returns
    -------
    pd.DataFrame
        Powerplant data filtered by year
    """
    ppl = pd.read_csv(powerplants_path, index_col=0, dtype={"bus": "str"})
    ppl = ppl[ppl.build_year == year]
    ppl["max_hours"] = 0  # Initialize max_hours column

    return ppl


def _integrate_renewables(
    n: pypsa.Network,
    electricity_config: dict[str, Any],
    renewable_config: dict[str, Any],
    costs: pd.DataFrame,
    renewable_profiles: dict[str, str],
    ppl: pd.DataFrame,
    hydro_capacities_path: str | None,
) -> None:
    """
    Integrate renewable generators into the network.

    Parameters
    ----------
    network : pypsa.Network
        Network to modify
    electricity_config : dict
        Electricity configuration dictionary
    renewable_config : dict
        Renewable configuration dictionary
    clustering_config : dict
        Clustering configuration dictionary
    line_length_factor : float
        Line length multiplication factor
    costs : pd.DataFrame
        Cost data
    renewable_profiles : dict
        Mapping of carrier names to profile file paths
    powerplants_path : str
        Path to powerplants CSV file
    hydro_capacities_path : str or None
        Path to hydro capacities CSV file
    """
    renewable_carriers = list(electricity_config["renewable_carriers"])
    extendable_carriers = electricity_config["extendable_carriers"]

    if not renewable_carriers:
        logger.info("No renewable carriers configured; skipping integration")
        return

    if "hydro" not in renewable_carriers:
        return

    non_hydro_carriers = [
        carrier for carrier in renewable_carriers if carrier != "hydro"
    ]
    non_hydro_profiles = {
        k: v for k, v in renewable_profiles.items() if k != "profile_hydro"
    }

    if renewable_profiles:
        attach_wind_and_solar(
            n,
            costs,
            ppl,
            non_hydro_profiles,
            non_hydro_carriers,
            extendable_carriers,
        )

    if "hydro" in renewable_carriers:
        hydro_cfg = copy.deepcopy(renewable_config["hydro"])
        carriers = hydro_cfg.pop("carriers")

        attach_hydro(
            n,
            costs,
            ppl,
            renewable_profiles["profile_hydro"],
            hydro_capacities_path,
            carriers,
            **hydro_cfg,
        )


def add_gb_components(
    n: pypsa.Network,
    context: CompositionContext,
) -> pypsa.Network:
    """
    Add GB-specific components and filter to target countries.

    Parameters
    ----------
    n : pypsa.Network
        Network to modify
    context : CompositionContext
        Composition context

    Returns
    -------
    pypsa.Network
        Modified network
    """
    if context.countries:
        keep = n.buses.country.isin(context.countries)
        drop = n.buses.index[~keep]
        if len(drop) > 0:
            logger.info("Removing %d buses outside target countries", len(drop))
            n.mremove("Bus", drop)

    meta = n.meta.setdefault("gb_model", {})
    if context.countries:
        meta["countries"] = list(context.countries)

    return n


def process_demand_data(
    annual_demand: str,
    clustered_demand_profile: str,
    eur_demand: pd.DataFrame,
    year: int,
) -> pd.DataFrame:
    """
    Process the demand data for a particular demand type

    Parameters
    ----------
    n : pypsa.Network
        Network to finalize
    demand_list: list[str]
        CSV paths for demand data for each demand type
    clustered_demand_profile_list: list[str]
        CSV paths for demand shape for each demand type
    eur_demand: pd.DataFrame
        European annual demand data
    year:
        Year used in the modelling
    """

    # Read the files
    demand = pd.read_csv(annual_demand, index_col=["bus", "year"])
    demand_all = pd.concat([demand, eur_demand])
    demand_profile = pd.read_csv(
        clustered_demand_profile, index_col=[0], parse_dates=True
    )

    # Group demand data by year and bus and filter the data for required year
    demand_this_year = demand_all.xs(year, level="year")

    # Filtering those buses that are present in both the dataframes
    if diff_bus := set(
        demand_this_year.index.get_level_values("bus")
    ).symmetric_difference(set(demand_profile.columns)):
        logger.warning(
            "The following buses are missing demand profile or annual demand data and will be ignored: %s",
            diff_bus,
        )

    # Scale the profile by the annual demand from FES
    load = demand_profile.drop(columns=diff_bus).mul(demand_this_year["p_set"])
    assert not load.isnull().values.any(), "NaN values found in processed load data"
    return load


def add_load(n: pypsa.Network, demands: dict[str, str]):
    """
    Add load as a timeseries to PyPSA network

    Parameters
    ----------
    n : pypsa.Network
        Network to finalize
    demands: dict[str, str]
        Mapping of demand types to CSV paths for demand data
    """
    # Iterate through each demand type
    for demand_type, demand_path in demands.items():
        if not demand_type.endswith("_demand"):
            raise ValueError(
                f"Unexpected demand type: {demand_type}. "
                "All demand types should start with 'demand_'."
            )
        # Process data for the demand type
        if demand_type == "ev_demand":
            add_EV_load(n, demand_path)
        else:
            load = pd.read_csv(demand_path, index_col=[0], parse_dates=True)
            # Add the load to pypsa Network
            suffix = f" {demand_type.removesuffix('_demand')}"
            n.add(
                "Load",
                load.columns + suffix,
                bus=load.columns,
                p_set=load.add_suffix(suffix),
            )


def add_EV_load(n: pypsa.Network, ev_demand_path: str):
    """
    Add EV load as a timeseries to PyPSA network

    Parameters
    ----------
    n : pypsa.Network
        Network to finalize
    ev_demand_path : str
        Path to EV demand CSV
    """
    # Compute EV demand profile using demand shape, annual EV demand and peak EV demand

    ev_demand = pd.read_csv(ev_demand_path, index_col=[0], parse_dates=True)

    # Add EV bus
    n.add(
        "Bus",
        ev_demand.columns,
        suffix=" EV",
        carrier="EV",
        x=n.buses.loc[ev_demand.columns].x,
        y=n.buses.loc[ev_demand.columns].y,
        country=n.buses.loc[ev_demand.columns].country,
    )

    # Add the EV load to pypsa Network
    n.add(
        "Load",
        ev_demand.columns,
        suffix=" EV",
        bus=ev_demand.columns + " EV",
        carrier="EV",
        p_set=ev_demand.add_suffix(" EV"),
    )

    # Add EV unmanaged charging
    n.add(
        "Link",
        ev_demand.columns,
        suffix=" EV unmanaged charging",
        bus0=ev_demand.columns,
        bus1=ev_demand.columns + " EV",
        p_nom=ev_demand.max(),
        efficiency=1.0,
        carrier="EV unmanaged charging",
    )


def _load_regional_data(path: str, year: int) -> pd.DataFrame:
    """
    Load regional data from CSV file and filter by year.
    """
    df = pd.read_csv(path, index_col=["bus", "year"])
    df_year = df.xs(year, level="year")
    return df_year


def add_EV_DSR_V2G(
    n: pypsa.Network,
    year: int,
    *,
    regional_ev_storage_inc_eur,
    regional_ev_dsm_inc_eur,
    regional_ev_v2g_inc_eur,
    dsm_profile_s_clustered,
):
    """
    Add EV DSR and V2G components to PyPSA network

    Parameters
    ----------
    n : pypsa.Network
        Network to finalize
    year: int
        Year used in the modelling
    regional_ev_storage_inc_eur: str
        CSV path for EV storage capacity
    regional_ev_dsm_inc_eur: str
        CSV path for EV smart charging (DSR) data
    regional_ev_v2g_inc_eur: str
        CSV path for EV V2G data
    dsm_profile_s_clustered: str
        CSV path for EV DSM profile

    """
    # Load EV storage data

    ev_storage_capacity_df = _load_regional_data(regional_ev_storage_inc_eur, year)
    ev_dsr_df = _load_regional_data(regional_ev_dsm_inc_eur, year)
    ev_v2g_df = _load_regional_data(regional_ev_v2g_inc_eur, year)
    # Add the EV store to pypsa Network
    ev_dsm_profile = pd.read_csv(dsm_profile_s_clustered, index_col=0, parse_dates=True)

    # Add EV storage buses
    n.add(
        "Bus",
        ev_storage_capacity_df.index,
        suffix=" EV store",
        carrier="EV store",
        x=n.buses.loc[ev_storage_capacity_df.index].x,
        y=n.buses.loc[ev_storage_capacity_df.index].y,
        country=n.buses.loc[ev_storage_capacity_df.index].country,
    )
    n.add(
        "Store",
        ev_storage_capacity_df.index,
        suffix=" EV store",
        bus=ev_storage_capacity_df.index + " EV store",
        e_nom=ev_storage_capacity_df.MWh,
        e_cyclic=True,
        carrier="EV store",
        e_min_pu=ev_dsm_profile.loc[:, ev_storage_capacity_df.index],
    )

    # Add the EV DSR to the PyPSA network
    n.add(
        "Link",
        ev_dsr_df.index,
        suffix=" EV DSR",
        bus0=ev_dsr_df.index + " EV store",
        bus1=ev_dsr_df.index + " EV",
        p_nom=ev_dsr_df.p_nom.abs(),
        efficiency=1.0,
        carrier="EV DSR",
    )
    n.add(
        "Link",
        ev_dsr_df.index,
        suffix=" EV DSR reverse",
        bus0=ev_dsr_df.index + " EV",
        bus1=ev_dsr_df.index + " EV store",
        p_nom=ev_dsr_df.p_nom.abs(),
        efficiency=1.0,
        carrier="EV DSR reverse",
    )

    # Add EV V2G to the PyPSA network
    n.add(
        "Link",
        ev_v2g_df.index,
        suffix=" EV V2G",
        bus0=ev_v2g_df.index + " EV store",
        bus1=ev_v2g_df.index,
        p_nom=ev_v2g_df.p_nom.abs(),
        efficiency=1.0,
        carrier="EV V2G",
    )


def _add_dsr_pypsa_components(
    n: pypsa.Network, df: pd.DataFrame, dsr_hours: list[int], key: str
):
    """
    Add DSR components for a given sector to PyPSA network

    Parameters
    ----------
        n : pypsa.Network
            Network to finalize
        df : pd.DataFrame
            DataFrame containing flexibility p_nom data indexed by bus
        dsr_hours : list[int]
            Hours during which demand-side management can occur
        key : str
            Sector key (e.g., 'residential', 'iandc', 'iandc_heat')
    """

    # Add the DSR carrier to the PyPSA network
    n.add(
        "Carrier",
        f"{key} DSR",
        nice_name=f"{key} Demand Side Response",
    )

    # Add the DSR shift and reverse carriers to the PyPSA network
    n.add(
        "Carrier",
        f"{key} DSR shift",
    )

    n.add(
        "Carrier",
        f"{key} DSR reverse",
    )

    # Create DSR buses, links and stores
    # Add the DSR bus to the PyPSA network
    n.add(
        "Bus",
        df.index,
        suffix=f" {key} DSR bus",
        carrier=f"{key} DSR",
        x=n.buses.loc[df.index].x,
        y=n.buses.loc[df.index].y,
        country=n.buses.loc[df.index].country,
    )

    # Add the DSR link from AC bus to DSR bus to the PyPSA network
    n.add(
        "Link",
        df.index,
        suffix=f" {key} DSR",
        bus0=df.index,
        bus1=df.index + f" {key} DSR bus",
        p_nom=df.p_nom.abs(),
        efficiency=1.0,
        carrier=f"{key} DSR shift",
    )

    # Add the DSR link from DSR bus to AC bus to the PyPSA network
    n.add(
        "Link",
        df.index,
        suffix=f" {key} DSR reverse",
        bus0=df.index + f" {key} DSR bus",
        bus1=df.index,
        p_nom=df.p_nom.abs(),
        efficiency=1.0,
        carrier=f"{key} DSR reverse",
    )

    breakpoint()
    # Create DSM profile to set as e_max_pu for DSR store
    dsr_profile = pd.DataFrame(index=n.snapshots, columns=df.index, data=0.0)
    mask = (dsr_profile.index.hour >= dsr_hours[0]) & (
        dsr_profile.index.hour < dsr_hours[1]
    )
    dsr_profile.loc[mask] = 1.0
    EU_columns = [col for col in df.index if "GB" not in col]
    # Shift European neighbour columns by 1 hour to account for time zone difference
    dsr_profile.loc[:, EU_columns] = dsr_profile.loc[:, EU_columns].shift(1, fill_value=0.0)

    # Calculate DSR duration in hours
    dsm_duration=dsr_hours[1]-dsr_hours[0]

    # Add the DSR store to the PyPSA network
    n.add(
        "Store",
        df.index,
        suffix=f" {key} DSR store",
        bus=df.index + f" {key} DSR bus",
        e_nom=df.p_nom.abs() * (dsm_duration),
        e_cyclic=True,
        carrier=f"{key} DSR",
        e_max_pu=dsr_profile,
    )


def add_DSR_baseline_heat(
    n: pypsa.Network,
    year: int,
    dsr: dict[str, str],
    dsr_hours: list[int],
):
    """
    Add DSR components for residential, i&c and i&c heat sectors to PyPSA network

    Parameters
    ----------
        n : pypsa.Network
            Network to finalize
        year: int
            Year used in the modelling
        dsr: dict[str, str]
            Dictionary containing DSR flexibility data for baseline and electrified heat
        dsr_hours: list[int]
            Hours during which demand-side management can occur
    """

    for file, path in dsr.items():
        if "residential" in file:
            df_dsr = _load_regional_data(path, year)
            _add_dsr_pypsa_components(n, df_dsr, dsr_hours, "residential")
        elif "iandc" in file and "heat" not in file:
            df_dsr = _load_regional_data(path, year)
            _add_dsr_pypsa_components(n, df_dsr, dsr_hours, "iandc")
        else:  # iandc_heat
            df_dsr = _load_regional_data(path, year)
            _add_dsr_pypsa_components(n, df_dsr, dsr_hours, "iandc_heat")


def finalise_composed_network(
    n: pypsa.Network,
    context: CompositionContext,
) -> pypsa.Network:
    """
    Finalize network composition with topology and consistency checks.

    Parameters
    ----------
    n : pypsa.Network
        Network to finalize
    context : CompositionContext
        Composition context

    Returns
    -------
    pypsa.Network
        Finalized network
    """
    n.determine_network_topology()
    meta = n.meta.setdefault("gb_model", {})
    meta["resources_root"] = str(context.resources_root)
    meta["composed"] = True
    n.consistency_check()
    return n


def attach_dc_interconnectors(
    n: pypsa.Network, interconnectors_path: str, year: int
) -> None:
    """
    Attach DC interconnector links to the network with fixed capacities.

    Removes existing DC links between GB and neighboring countries, then adds
    new links based on the interconnectors CSV file for the specified year.

    Args:
        n (pypsa.Network): The PyPSA network
        interconnectors_path (str): Path to interconnectors CSV file
        year (int): Year for which to load interconnector capacities
    """
    # Load interconnector data
    interconnectors = pd.read_csv(
        interconnectors_path,
        dtype={"bus0": str, "bus1": str},
        index_col=["project", "year"],
    )
    interconnectors_this_year = interconnectors.xs(year, level="year")

    if interconnectors_this_year.empty:
        logger.warning(f"No interconnector data found for year {year}")
        return

    # Find DC links connecting GB to neighbors (bidirectional)
    existing_dc_links = n.links[
        (n.links.carrier == "DC")
        & (
            (n.links.bus0.str.startswith("GB") & ~n.links.bus1.str.startswith("GB"))
            | (n.links.bus1.str.startswith("GB") & ~n.links.bus0.str.startswith("GB"))
        )
    ]

    # Remove existing DC interconnectors
    if not existing_dc_links.empty:
        removed_links = existing_dc_links.index.tolist()
        n.remove("Link", removed_links)
        logger.info(
            f"Removed {len(removed_links)} existing DC interconnector links: {removed_links}"
        )
    else:
        logger.info("No existing DC interconnector links found to remove")

    # Add the links
    n.add("Link", interconnectors_this_year.index, **interconnectors_this_year)

    logger.info(
        f"Added {len(interconnectors_this_year)} DC interconnector links with total capacity "
        f"{interconnectors_this_year['p_nom'].sum():.1f} MW for year {year}"
    )


def attach_chp_constraints(n: pypsa.Network, p_min_pu: pd.DataFrame) -> None:
    """
    Attach CHP operating constraints to the network.

    Args:
        n (pypsa.Network): The PyPSA network
        p_min_pu (pd.DataFrame): Minimum operation profile for CHP generators
    """
    chp_generators = n.generators[n.generators["set"] == "CHP"]

    if chp_generators.empty:
        logger.info(
            "No CHP generators found in the network. "
            f"Total generators: {len(n.generators)}, "
            f"generators by set: {n.generators.groupby('set').size().to_dict()}"
        )
        return

    logger.info(
        f"Applying CHP constraints to {len(chp_generators)} generators "
        f"with total capacity {chp_generators.p_nom.sum():.1f} MW"
    )

    # Map minimum operation to generators (vectorized)
    # Each generator inherits the profile of its bus
    gen_to_bus = chp_generators["bus"]

    # Filter to only generators with available heat demand data
    valid_gens = gen_to_bus[gen_to_bus.isin(p_min_pu.columns)]
    missing_gens = gen_to_bus[~gen_to_bus.isin(p_min_pu.columns)]

    if not missing_gens.empty:
        logger.warning(
            f"No heat demand data for {len(missing_gens)} generators at buses: {list(missing_gens.unique())}. "
            "These generators will have no CHP constraint."
        )

    # Vectorized assignment: rename p_min_pu columns from bus names to generator indices
    # Select columns for each generator's bus, then rename to generator index
    p_min_pu_for_gens = p_min_pu[valid_gens.values].copy()
    p_min_pu_for_gens.columns = valid_gens.index

    # Assign all generators at once
    _add_timeseries_data_to_network(n.generators_t, p_min_pu_for_gens, "p_min_pu")


def attach_wind_and_solar(
    n: pypsa.Network,
    costs: pd.DataFrame,
    ppl: pd.DataFrame,
    profile_filenames: dict,
    carriers: list | set,
    extendable_carriers: list | set,
) -> None:
    """
    Attach wind and solar generators to the network.

    Parameters
    ----------
    n : pypsa.Network
        The PyPSA network to attach the generators to.
    costs : pd.DataFrame
        DataFrame containing the cost data.
    ppl : pd.DataFrame
        DataFrame containing the power plant data.
    profile_filenames : dict
        Dictionary containing the paths to the wind and solar profiles.
    carriers : list | set
        List of renewable energy carriers to attach.
    extendable_carriers : list | set
        List of extendable renewable energy carriers.
    """
    add_missing_carriers(n, carriers)

    for car in carriers:
        if car == "hydro":
            continue

        with xr.open_dataset(profile_filenames["profile_" + car]) as ds:
            if ds.indexes["bus"].empty:
                continue

            # if-statement for compatibility with old profiles
            if "year" in ds.indexes:
                ds = ds.sel(year=ds.year.min(), drop=True)

            ds = ds.stack(bus_bin=["bus", "bin"])

            supcar = car.split("-", 2)[0]
            capital_cost = costs.at[supcar, "capital_cost"]

            buses = ds.indexes["bus_bin"].get_level_values("bus")
            bus_bins = ds.indexes["bus_bin"].map(flatten)

            p_nom_max = ds["p_nom_max"].to_pandas()
            p_nom_max.index = p_nom_max.index.map(flatten)

            p_max_pu = ds["profile"].to_pandas()
            p_max_pu.columns = p_max_pu.columns.map(flatten)

            if not ppl.query("carrier == @supcar").empty:
                caps = ppl.query("carrier == @supcar").groupby("bus").p_nom.sum()
                caps = caps.reindex(buses).fillna(0)
                caps = pd.Series(data=caps.values, index=bus_bins)
            else:
                caps = pd.Series(index=bus_bins).fillna(0)

            n.add(
                "Generator",
                bus_bins,
                suffix=" " + supcar,
                bus=buses,
                carrier=supcar,
                p_nom=caps,
                p_nom_min=caps,
                p_nom_extendable=car in extendable_carriers["Generator"],
                p_nom_max=p_nom_max,
                marginal_cost=costs.at[supcar, "marginal_cost"],
                capital_cost=capital_cost,
                efficiency=costs.at[supcar, "efficiency"],
                p_max_pu=p_max_pu,
                lifetime=costs.at[supcar, "lifetime"],
            )


def _prepare_costs(
    ppl: pd.DataFrame,
    year: int,
) -> pd.DataFrame:
    """
    Prepare costs DataFrame from powerplant data.
    """
    costs = ppl[ppl.build_year == year]
    costs = costs[~costs.set_index("carrier").index.duplicated(keep="first")].set_index(
        "carrier"
    )
    costs = costs[
        [
            "set",
            "capital_cost",
            "marginal_cost",
            "lifetime",
            "efficiency",
            "CO2 intensity",
        ]
    ]
    return costs


def _prune_lines(
    n: pypsa.Network,
    prune_lines: list[dict[str, int]],
) -> None:
    """
    Prune lines between bus0 and bus1 by setting their s_max_pu to 0.

    Args:
        n (pypsa.Network): The PyPSA network to modify.
        prune_lines (list[dict[str, int]]): The lines to prune.
    """
    # Prune specified lines
    for line in prune_lines:
        mask = get_lines(n.lines, line["bus0"], line["bus1"])
        if mask.any():
            # Get lines to remove
            lines_to_remove = n.lines[mask].index

            # Remove the lines from the network
            n.remove("Line", lines_to_remove)
            logger.info(
                f"Deleted line between bus {line['bus0']} and bus {line['bus1']}"
            )
        else:
            logger.warning(
                f"No line found to prune between bus {line['bus0']} and bus {line['bus1']}"
            )


def compose_network(
    network_path: str,
    output_path: str,
    costs_path: str,
    powerplants_path: str,
    hydro_capacities_path: str | None,
    chp_p_min_pu_path: str,
    interconnectors_path: str,
    renewable_profiles: dict[str, str],
    countries: list[str],
    costs_config: dict[str, Any],
    electricity_config: dict[str, Any],
    renewable_config: dict[str, Any],
    demands: dict[str, str],
    ev_data: dict[str, str],
    prune_lines: list[dict[str, int]],
    dsr: dict[str, str],
    enable_chp: bool,
    dsr_hours: list[int],
    year: int,
) -> None:
    """
    Main composition function to create GB market model network.

    Parameters
    ----------
    network_path : str
        Path to input base network
    output_path : str
        Path to save composed network
    costs_path : str
        Path to costs CSV file
    powerplants_path : str
        Path to powerplants CSV file
    hydro_capacities_path : str or None
        Path to hydro capacities CSV file
    chp_p_min_pu_path : str
        Path to CHP minimum operation profile CSV file
    interconnectors_path : str
        Path to interconnectors CSV file with DC link capacities
    renewable_profiles : dict
        Mapping of carrier names to profile file paths
    heat_demand_path : str
        Path to hourly heat demand NetCDF file for CHP constraints
    countries : list[str]
        List of country codes to include
    costs_config : dict
        Costs configuration dictionary
    electricity_config : dict
        Electricity configuration dictionary
    clustering_config : dict
        Clustering configuration dictionary
    renewable_config : dict
        Renewable configuration dictionary
    demands: dict[str, str]
        Dictionary mapping demand types to paths for the demand data
    ev_data : dict[str, str]
        Dictionary containing EV flexibility data
    prune_lines : list[dict[str, int]]
        List of lines to prune between specified bus pairs
    dsr : dict[str, str]
        Dictionary containing DSR flexibility data for baseline and electrified heat
    enable_chp : bool
        Whether to enable CHP constraints
    year: int
        Modelling year
    """
    network = pypsa.Network(network_path)
    max_hours = electricity_config["max_hours"]
    context = create_context(
        network_path, costs_path, countries, costs_config, max_hours
    )
    add_gb_components(network, context)

    # Load FES powerplants data (already enriched with costs from create_powerplants_table)
    ppl = _load_powerplants(powerplants_path, year)

    # Define costs file
    costs = _prepare_costs(ppl, year)

    _integrate_renewables(
        network,
        electricity_config,
        renewable_config,
        costs,
        renewable_profiles,
        ppl,
        hydro_capacities_path,
    )

    conventional_carriers = list(electricity_config["conventional_carriers"])

    attach_conventional_generators(
        network,
        costs,
        ppl,
        conventional_carriers,
        extendable_carriers={"Generator": []},
        conventional_params={},
        conventional_inputs={},
        unit_commitment=None,
    )

    # Add simplified CHP constraints if enabled
    if enable_chp:
        logger.info("Adding simplified CHP constraints based on heat demand.")
        chp_p_min_pu = pd.read_csv(
            chp_p_min_pu_path, index_col="snapshot", parse_dates=True
        )
        attach_chp_constraints(network, chp_p_min_pu)

    add_load(network, demands)

    add_EV_DSR_V2G(network, year, **ev_data)

    attach_dc_interconnectors(network, interconnectors_path, year)

    _prune_lines(network, prune_lines)

    add_DSR_baseline_heat(network, year, dsr, dsr_hours)

    finalise_composed_network(network, context)

    network.export_to_netcdf(output_path)


if __name__ == "__main__":
    if "snakemake" not in globals():
        from scripts._helpers import mock_snakemake

        snakemake = mock_snakemake("compose_network", clusters="clustered", year=2022)

    configure_logging(snakemake)
    set_scenario_config(snakemake)
    log_suffix = "-" + "_".join(snakemake.wildcards) if snakemake.wildcards else ""
    logger = logging.getLogger(Path(__file__).stem + log_suffix)

    # Extract renewable profiles from inputs
    renewable_carriers = snakemake.params.electricity["renewable_carriers"]
    renewable_profile_keys = [f"profile_{carrier}" for carrier in renewable_carriers]
    renewable_profiles = {key: snakemake.input[key] for key in renewable_profile_keys}

    compose_network(
        network_path=snakemake.input.network,
        output_path=snakemake.output.network,
        costs_path=snakemake.input.tech_costs,
        powerplants_path=snakemake.input.powerplants,
        hydro_capacities_path=snakemake.input.hydro_capacities,
        renewable_profiles=renewable_profiles,
        chp_p_min_pu_path=snakemake.input.chp_p_min_pu,
        interconnectors_path=snakemake.input.interconnectors_p_nom,
        countries=snakemake.params.countries,
        costs_config=snakemake.params.costs_config,
        electricity_config=snakemake.params.electricity,
        renewable_config=snakemake.params.renewable,
        demands=_input_list_to_dict(snakemake.input.demands, parent=True),
        ev_data=_input_list_to_dict(snakemake.input.ev_data),
        dsr=_input_list_to_dict(snakemake.input.dsr),
        enable_chp=snakemake.params.enable_chp,
        prune_lines=snakemake.params.prune_lines,
        dsr_hours=snakemake.params.dsr_hours,
        year=int(snakemake.wildcards.year),
    )
